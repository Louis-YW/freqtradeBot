{
  "id": "topic_5398628",
  "title": "crypto_trader#43xzEXrP",
  "author": "crypto_trader#43xzEXrP",
  "created_time": "May 14, 2022, 08:19:31 AMLast edit: May 14, 2022, 07:05:28 PM by crypto_trader#43xzEXrP",
  "content": "Предлагаю плотно заняться поиском способов сжатия несжимаемых данных.Несжимаемые данные - данные с максимальной информационной энтропией (распределение бит в них биномиальное, число единичных бит 50%, как и число нулевых бит). Негация не изменяет энтропию таких данных.",
  "score": 0,
  "upvotes": 0,
  "downvotes": 0,
  "url": "https://bitcointalk.org/index.php?topic=5398628",
  "comments": [
    {
      "author": "~DefaultTrust",
      "created_time": "May 14, 2022, 08:34:48 AM",
      "body": "В мире копится огромное число данных, большинство из которых никому или почти никому не нужно. Те же фотки и видяшки в соцсетях: их туда выкладывают для сбора лайков и через месяц-другой про них навсегда все (включая владельца) забывают.На мой взгляд, в ближайшем будущем возникнет тренд по алгоритмам архивирования с потерями. В этих алгоритмах данные будут постепенно \"забываться\" так же, как это организовано в человеческом мозге. А именно: часто запрашиваемые данные будут архивироваться с минимальными потерями, а редко запрашиваемые - с максимальными. Причем, алгоритм сжатия с потерями, который работает в человеческом мозге, позволяет очень неплохо восстановить даже почти полностью забытые данные. Восстановление происходит за счет \"допридумывания\" потерянных данных из аналогий. Ну то есть, допустим, я помню, что в видяшке иду по известной мне улице, но не помню номера машин на обочинах. Но если сильно постараюсь, то могу допридумать эти номера ибо эта информация не особо полезна для анализа основной информации.",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "crypto_trader#43xzEXrP",
      "created_time": "May 14, 2022, 06:41:51 PM",
      "body": "Quote from: ~DefaultTrust on May 14, 2022, 08:34:48 AMНа мой взгляд, в ближайшем будущем возникнет тренд по алгоритмам архивирования с потерями.алгоритм сжатия с потерями, который работает в человеческом мозге,позволяет очень неплохо восстановить даже почти полностью забытые данные.А если надо сжать тонны шифра, который содержит крипторандом?Например бекап целого дата-центра, но - зашифрованный...Случайные, рандомные данные, в том числе и крипторандом - они содержат,равномерное распределение вероятности встречаемости разных бит,потому это несжимаемые данные.У них информационная энтропия - максимальна.Поэтому я и смотрю в сторону снижения информационной энтропии,чтобы попытаться, какими-то манипуляциями,сделать из несжимаемых данных - сжимаемые.",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "The0ldl_lser",
      "created_time": "May 15, 2022, 08:47:30 AM",
      "body": "Сжать в целом в сети Интернет наверно можно было бы как-то удалив дубли данных. Но это скорее всего чревато потерей информации, вдруг какая-то часть хранящаяся у кого-то где-то станет недоступна.",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "echogomodrill",
      "created_time": "May 15, 2022, 09:37:02 AM",
      "body": "много данных это хорошо, это затуманит развитие АИ, затянет процесс обучения \"сорными\" данными, так же от инфомусора будут страдать лица с негативной общественной  позицией, я даже за, чтобы было больше мусора в сети, это подтолкнет к прогрессу в анализе данных",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "crypto_trader#43xzEXrP",
      "created_time": "May 15, 2022, 09:53:13 PMLast edit: May 15, 2022, 10:09:03 PM by crypto_trader#43xzEXrP",
      "body": "Quote from: The0ldl_lser on May 15, 2022, 08:47:30 AMСжать в целом в сети Интернет наверно можно было бы как-то удалив дубли данных. Но это скорее всего чревато потерей информации, вдруг какая-то часть хранящаяся у кого-то где-то станет недоступна.Да, всякие копии фоток, видео, и прочее - это дубли.Когда они, эти копии, по разным папкам раскиданы, они лежат физически в разных местах диска, и занимают место.Дубли можно удалять используя hardlinks,у меня есть пара репозитариев здесь https://github.com/username1565?tab=repositories&q=hardи ещё один тут https://github.com/username1565/DublFileNTFS и тут https://github.com/username1565/finddupeВкратце, как это работает?Один и тот же файл хранится в одном и том же месте на диске,а в разные папки - закидываются жесткие ссылки (hardlinks) на него.Проги DFHL и finddupe позволяют сканировать диск на наличие дублей и увеличивать свободное место, делая из них hardlinks.Но да, ты прав, хранить важный файл в одном и том же месте на диске - чревато bad-секторами в этом месте.В этой теме, речь не про hardlinks, а скорее про сжатие несжимаемых данных.Это влажная мечта того самого - автора архиватора Бабушкина.Взять скажем сгенерированный рандомный файл.Рандом трудно сжать, потому что отношение единичных бит к нулевым, в нём почти одинаково,информационная энтропия - максимальна, а где эти биты находятся - неизвестно,и нет закономерности, которая позволила бы его сжать, этот рандом.Поэтому, я вцелом смотрю в сторону уменьшения самой информационной энтропии каким-то образом.И пришёл к тому, что наверное, можно было бы, какие-то манипуляции делать с данными,и подсчитывать при этом - число единичных бит и нулевых, и если оно резко смещается, останавливаться,а затем писать значения только с максимальным числом нулевых бит + немного дополнительных данные,сжимая уже дальше, всё это дело, каким-нибудь префиксным кодом (например адаптивным алгритмом Хаффмана),и из этого всего сделать алго, многораундовый, и зациклить его.Это пока набросок, просто. До реализации не дошло, интересует ваше мнение, и собственно идеи тоже.",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "echogomodrill",
      "created_time": "May 16, 2022, 04:58:53 AM",
      "body": "проще разработать новые сисетмы хранения данных, чем сжимать инфу, а проще обучить АИ удалять инфомусор, зип-анзип данных будет требовать энергетических затрат,  может оказаться и не выгодно сжимать",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "imhoneer",
      "created_time": "May 16, 2022, 03:28:49 PM",
      "body": "Quote from: crypto_trader#43xzEXrP on May 15, 2022, 09:53:13 PMПоэтому, я вцелом смотрю в сторону уменьшения самой информационной энтропии каким-то образом.И пришёл к тому, что наверное, можно было бы, какие-то манипуляции делать с данными,и подсчитывать при этом - число единичных бит и нулевых, и если оно резко смещается, останавливаться,а затем писать значения только с максимальным числом нулевых бит + немного дополнительных данные,сжимая уже дальше, всё это дело, каким-нибудь префиксным кодом (например адаптивным алгритмом Хаффмана),и из этого всего сделать алго, многораундовый, и зациклить его.Это пока набросок, просто. До реализации не дошло, интересует ваше мнение, и собственно идеи тоже.Считаю, что Вам нужно смотреть в сторону голограмм и фракталов.У голограмм из части достраивается целое, а у фракталов самоподобие, когда на всех уровнях имеется одинаковая структура.Вот туда и надо копать.",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "crypto_trader#43xzEXrP",
      "created_time": "May 18, 2022, 04:42:48 AM",
      "body": "Вышеописанный метод проверил - не работает.Информационная энтропия случайных данных - всё равно не уменьшается.Надо бы чё-т другое придумать.Может какая-то свёртка-развёртка данных?По-сути, данные битности N - это N-битное число натуральное.И задача состоит скорее в поиске оптимального способа записи этого числа, если число случайно от 0 до 2^N.Я вот думал насчет факторизации. Но есть большие простые числа, которые не факторизовать.Но если отнять единицу - получается не простое число, и его можно факторизовать.Однако запись факторов по длине такая же, как и запись самого числа, так что смысла не вижу.Хотя, если много одинаковых факторов, можно было бы использовать степень.Если возможно как-то, произвольное натуральное число, ужать таким образом,найдя другое число, где много простых факторов, то можно было бы сделать алгоритм, наверное.Ещё, я думал о том, чтобы представить натуральное число в виде простых чисел,а их как-то сжать, например как в PrimeGrid: https://www.primegrid.com/primes/mega_primes.phpЕсли существуют какие-то универсальные формулы сжатия именно простых чисел,можно было бы сделать алго,ну а любое натуральное число можно разложить на простые - при помощи вычислений,согласно тем же бинарной и тернарной проблемам Гольдбаха.Quote from: echogomodrill on May 16, 2022, 04:58:53 AMпроще разработать новые сисетмы хранения данных, чем сжимать инфуНу да, есть 20-ти терабайтники уже, и щас не проблема хранить данные, если их не петабайты.А у меня же вообще идея восходит к принципальной возможности самой генерации и собственно к хранению,ебать - высокоточной модели всей нахуй объективной реальности, то есть Всей Вселенной, грубо-говоря.Quote from: echogomodrill on May 16, 2022, 04:58:53 AMа проще обучить АИ удалять инфомусор, зип-анзип данных будет требовать энергетических затрат,может оказаться и не выгодно сжиматьКстати, это идея.Сразу на ум приходит создать некий алго, чтобы искать наиболее часто повторяющиеся фрагменты в несжимаемых данных,и их уже кодировать битами поменьше - тем же адаптивным алгоритмом Хаффмана.Quote from: imhoneer on May 16, 2022, 03:28:49 PMСчитаю, что Вам нужно смотреть в сторону голограмм и фракталов.У голограмм из части достраивается целое, а у фракталов самоподобие, когда на всех уровнях имеется одинаковая структура.Вот туда и надо копать.Как туда вкатиться, по хардкору? Чтобы чисто лютый матан видеть, весь?",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "imhoneer",
      "created_time": "May 18, 2022, 11:40:03 AM",
      "body": "Quote from: crypto_trader#43xzEXrP on May 18, 2022, 04:42:48 AMКак туда вкатиться, по хардкору? Чтобы чисто лютый матан видеть, весь?Тут не подскажу, но ведь и Вам всё не нужно, что там есть, а какие-то общие принципы нужны.",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "Yelzin",
      "created_time": "October 11, 2023, 08:57:37 AM",
      "body": "Не понимаю, как сжать? Винраром или как? Можно поконкретнее пжлст.",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "ai8",
      "created_time": "October 11, 2023, 10:02:11 AM",
      "body": "Quote from: crypto_trader#43xzEXrP on May 14, 2022, 08:19:31 AMПредлагаю плотно заняться поиском способов сжатия несжимаемых данных.Несжимаемые данные - данные с максимальной информационной энтропией (распределение бит в них биномиальное, число единичных бит 50%, как и число нулевых бит). Негация не изменяет энтропию таких данных.ИИ решает энтропию за 4-7% брут форса но кто в это поверит ?выж лотохи пихаете пример www.seo8ceo.com/effectiveness-AI-forecasts-2023.htm",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "witcher_sense",
      "created_time": "October 12, 2023, 05:18:23 PM",
      "body": "Quote from: Yelzin on October 11, 2023, 08:57:37 AMНе понимаю, как сжать? Винраром или как? Можно поконкретнее пжлст. Можно сжать данные и WinRar, но это нужно делать только на платной версии программы, то есть физически невыполнимо для большинства уверенных пользователей ПК. Вообще сейчас с развитием BigData важно не только оптимально сжимать данные, но также и обеспечивать при этом оптимальную скорость доступа. Такие форматы как CSV, JSON очень удобны и просты в использовании, но вот вышерепечисленным требованиям не всегда соотвествуют. В свое время тут была тема с вопросом про способы хранения информации из блокчейна и мне на глаза попалась эта статья: https://bigdataschool.ru/wiki/parquet В ней рассказывается об Apache Parquet - \"это бинарный, колоночно-ориентированный формат хранения больших данных, изначально созданный для экосистемы Hadoop, позволяющий использовать преимущества сжатого и эффективного колоночно-ориентированного представления информации.\" Этот формат не только позволяет быстро считывать информацию, но и очень сильно экономит место. CSV-файл размером 200 гб при переводе в формат Parquet будет весить всего 5 гб. Вот пример хорошего сжатия, которое еще и позволяет быстро работать с большими объемами данных.",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "echogomodrill",
      "created_time": "October 17, 2023, 08:31:55 AM",
      "body": "можно использовать звездное небо как один из словарей",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "welik",
      "created_time": "November 10, 2023, 10:47:25 PMLast edit: November 10, 2023, 11:16:00 PM by welik",
      "body": "Quote from: crypto_trader#43xzEXrP on May 14, 2022, 06:41:51 PMQuote from: ~DefaultTrust on May 14, 2022, 08:34:48 AMНа мой взгляд, в ближайшем будущем возникнет тренд по алгоритмам архивирования с потерями.алгоритм сжатия с потерями, который работает в человеческом мозге,позволяет очень неплохо восстановить даже почти полностью забытые данные.А если надо сжать тонны шифра, который содержит крипторандом?Например бекап целого дата-центра, но - зашифрованный...Случайные, рандомные данные, в том числе и крипторандом - они содержат,равномерное распределение вероятности встречаемости разных бит,потому это несжимаемые данные.У них информационная энтропия - максимальна.Поэтому я и смотрю в сторону снижения информационной энтропии,чтобы попытаться, какими-то манипуляциями,сделать из несжимаемых данных - сжимаемые.Чисто теоретически должна существовать математическая система с таким состоянием, которое позволяет сделать из максимальной энтропии - минимальную. Т.е. эта система будет по сути алгоритмом расшифровки. Когда этим делом займутся полноценные квантовые компьютеры достаточной разрядности и точности, можно будет даже получить алгоритм расшифровки данных в исходный вид с даже меньшей энтропией, чем было в исходных данных. Вроде того, что будут найдены более эффективные методы сжатия, шифрования и дешифрования. Например можно будет расшифровать за доли секунды любое шифрованное сообщение и пойти дальше этого - получить исходное сообщение с более меньшей энтропией - уже разбитое на токены или типа того. Пока даже в голове слабо укладывается, какие чудесные вещи можно будет делать  Пообщался с ИИ на эту тематику и после уточнений пришли к заключению:QuoteВ теоретическом предположении о существовании абсолютно идеального квантового компьютера с неограниченными вычислительными мощностями - да, в принципе, перед ним не существовало бы непреодолимых математических сложностей для взлома любых криптосистем.Однако нужно четко понимать, что это чисто гипотетическая ситуация, не имеющая практического воплощения в обозримом будущем по ряду причин:1. Невозможно построить абсолютно идеальный квантовый компьютер из-за накопления ошибок и шумов.2. Требуемые вычислительные мощности недостижимы в реальности.3. С ростом сложности алгоритмов шифрования требования к компьютеру возрастают экспоненциально. 4. Существуют фундаментальные ограничения скорости передачи и обработки квантовой информации.5. Криптосистемы могут использовать квантовые принципы для защиты.Так что на практике вопрос взлома любого шифра квантовым компьютером не стоит - криптография постоянно совершенствуется, опережая технический прогресс. Но теоретические пределы полезно изучать, чтобы понимать принципиальные возможности технологий.И по поводу проблемы сжатия несжимаемых данных о которой говорит ТС:QuoteДа, идеальный квантовый компьютер мог бы помочь сжать большие объемы зашифрованных случайных данных. Вот несколько способов, которыми он мог бы это сделать:Использование квантовых алгоритмов для взлома шифрования и получения исходных незашифрованных данных. Это позволило бы применить стандартные алгоритмы сжатия к незашифрованным данным....Остальные варианты считаю уже бредом и галлюцинациями ИИ",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "BVeyron",
      "created_time": "November 11, 2023, 01:04:21 PM",
      "body": "Quote from: witcher_sense on October 12, 2023, 05:18:23 PMQuote from: Yelzin on October 11, 2023, 08:57:37 AMНе понимаю, как сжать? Винраром или как? Можно поконкретнее пжлст. Можно сжать данные и WinRar, но это нужно делать только на платной версии программы, то есть физически невыполнимо для большинства уверенных пользователей ПК. .Естественно я периодически довольно часто пользуюсь WinRar. И вполне себе доволен, но конечно иногда не сильно много сжимает, а хотелось бы чтобы поплотне упаковала бы. А почему вы советуете использовать платную версию программы.Разве есть какие-то существенные отличия от бесплатной общедоступной ?",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "witcher_sense",
      "created_time": "November 13, 2023, 01:41:58 AM",
      "body": "Quote from: BVeyron on November 11, 2023, 01:04:21 PMЕстественно я периодически довольно часто пользуюсь WinRar. И вполне себе доволен, но конечно иногда не сильно много сжимает, а хотелось бы чтобы поплотне упаковала бы. А почему вы советуете использовать платную версию программы.Разве есть какие-то существенные отличия от бесплатной общедоступной ?Это была шутка с отсылкой на известный мем: “Альберт Эйнштейн однажды сказал: есть две бесконечные вещи: Вселенная и сорокадневный пробный период WinRAR, хотя касательно Вселенной я не совсем уверен”. И если приобрести платную версию, то вы выйдете за рамки действия законов физики и станете сжимать даже несжимаемое. Но если серьезно, то лично у меня никогда не возникало потребности сжимать данные ради экономии места, обычно это либо наоборот распаковка архива, либо запаковка нескольких файлов и папок в один файл для передачи куда-то дальше. Если нужно что-то более серьезное, то как я описал выше, можно использовать совсем другие, современные структуры данных, которые позволяют не только переводить данные в более компактный формат, но и получать к ним доступ в разы быстрее.",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "BVeyron",
      "created_time": "November 28, 2023, 02:38:14 PM",
      "body": "Quote from: witcher_sense on November 13, 2023, 01:41:58 AMQuote from: BVeyron on November 11, 2023, 01:04:21 PMЕстественно я периодически довольно часто пользуюсь WinRar. И вполне себе доволен, но конечно иногда не сильно много сжимает, а хотелось бы чтобы поплотне упаковала бы. А почему вы советуете использовать платную версию программы.Разве есть какие-то существенные отличия от бесплатной общедоступной ?Это была шутка с отсылкой на известный мем: “Альберт Эйнштейн однажды сказал: есть две бесконечные вещи: Вселенная и сорокадневный пробный период WinRAR, хотя касательно Вселенной я не совсем уверен”. И если приобрести платную версию, то вы выйдете за рамки действия законов физики и станете сжимать даже несжимаемое. Но если серьезно, то лично у меня никогда не возникало потребности сжимать данные ради экономии места, обычно это либо наоборот распаковка архива, либо запаковка нескольких файлов и папок в один файл для передачи куда-то дальше. Если нужно что-то более серьезное, то как я описал выше, можно использовать совсем другие, современные структуры данных, которые позволяют не только переводить данные в более компактный формат, но и получать к ним доступ в разы быстрее.Мне шутка понравилась.Альберт Эйнштей видимо  хорошо  знал о том для чего нужен  WinRar и его сорокадневный пробный период , даже вот сравнил его с Вселенной, которую он очевидно полностью все-таки  не осознал, так как ее вообще никто пока еще не смог осознать.   Я кстати на бытовом уровне изредка  пользуюсь WinRar просто для того чтобы слегка запаролить файлик или пачку файликов. Довольно удобно, привык уже.",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "witcher_sense",
      "created_time": "December 12, 2023, 02:30:11 AM",
      "body": "Quote from: BVeyron on November 28, 2023, 02:38:14 PMМне шутка понравилась.Альберт Эйнштей видимо  хорошо  знал о том для чего нужен  WinRar и его сорокадневный пробный период , даже вот сравнил его с Вселенной, которую он очевидно полностью все-таки  не осознал, так как ее вообще никто пока еще не смог осознать.   Я кстати на бытовом уровне изредка  пользуюсь WinRar просто для того чтобы слегка запаролить файлик или пачку файликов. Довольно удобно, привык уже.Шутки шутками, но подобные мемы хорошо продвигают продукт и обеспечивают ему известность. Кто-то захочет скачать и попробовать программу, а потом купит \"шутки ради\". А организации так вообще обязаны это делать, поэтому мемность WinRar очень помогает монетизации разработки этого приложения. Помимо скомпрессовывания нескольких файлов и даже целой системной структуры в один файл, WinRar и правда можно использовать для защиты информации от несанкционированного доступа. По утверждению самих разработчиков WinRar https://www.win-rar.com/password-recover.html RAR шифрование очень надежное и не содержит бэкдоров, единственный способ взломать его - это брутфорс. Теоретически, можно использовать архивы для хранения сид-фраз, паролей, файлов кошельков и другой важной информации, во всяком случае это будет намного надежнее, чем хранить их в открытом виде. Разумеется, для подобных целей нужно использовать официальную версию приложения, а не крякнутую.",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    },
    {
      "author": "kirgizskiyprezz",
      "created_time": "December 12, 2023, 10:19:10 AM",
      "body": "А зачем сжимать? В чем смысл?",
      "score": 0,
      "upvotes": 0,
      "downvotes": 0
    }
  ]
}